/**
 * LLM æ¨¡å‹è°ƒç”¨å°è£…
 * ç»Ÿä¸€ç®¡ç†æœ¬åœ°æ¨¡å‹å’Œç«å±±å¼•æ“æ¨¡å‹çš„è°ƒç”¨
 */

import { volcengineService, type VolcengineMessage } from './volcengine-service.js';
import type { ChatMessage } from '../../../types/chat.js';

/**
 * è°ƒç”¨æœ¬åœ° Ollama æ¨¡å‹
 */
export async function callLocalModel(messages: ChatMessage[]) {
  const fetch = (await import('node-fetch')).default;
  const modelName = process.env.OLLAMA_MODEL || 'deepseek-r1:7b';
  const ollamaUrl = process.env.OLLAMA_API_URL || 'http://localhost:11434';
  
  const response = await fetch(`${ollamaUrl}/api/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model: modelName,
      messages,
      stream: true,
      keep_alive: '30m', // ä¿æŒæ¨¡å‹åœ¨å†…å­˜ä¸­ 30 åˆ†é’Ÿï¼Œé¿å…é¢‘ç¹é‡æ–°åŠ è½½
      // å¼ºåˆ¶ä½¿ç”¨ GPU - æ‰€æœ‰å±‚éƒ½åŠ è½½åˆ° GPU
      options: {
        num_gpu: 999,  // å¼ºåˆ¶æ‰€æœ‰å±‚ä½¿ç”¨ GPUï¼ˆ999 è¡¨ç¤ºå°½å¯èƒ½å¤šï¼‰
      }
    }),
  });

  if (!response.ok) {
    throw new Error(`Ollama API é”™è¯¯: ${response.statusText}`);
  }

  return response.body;
}

/**
 * è°ƒç”¨ç«å±±å¼•æ“è±†åŒ…å¤§æ¨¡å‹
 */
export async function callVolcengineModel(messages: ChatMessage[]) {
  // è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼ˆä¿æŒå…¼å®¹ï¼‰
  const volcengineMessages: VolcengineMessage[] = messages.map(msg => ({
    role: msg.role,
    content: msg.content,
  }));

  console.log('ğŸ”¥ è°ƒç”¨ç«å±±å¼•æ“è±†åŒ…æ¨¡å‹...');
  const stream = await volcengineService.chat(volcengineMessages, {
    temperature: 0.7,
    maxTokens: 4000,
    topP: 0.95,
  });

  return stream;
}

