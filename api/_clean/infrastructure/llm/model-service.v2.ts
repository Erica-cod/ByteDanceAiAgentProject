/**
 * æ¨¡å‹è°ƒç”¨æœåŠ¡ V2 - æ”¯æŒ Function Calling
 * ç»Ÿä¸€ç®¡ç†æœ¬åœ°æ¨¡å‹å’Œç«å±±å¼•æ“æ¨¡å‹çš„è°ƒç”¨ï¼Œæ”¯æŒå·¥å…·å®šä¹‰ä¼ é€’
 */

import { volcengineService, type VolcengineMessage } from './volcengine-service.js';
import type { ChatMessage } from '../../../types/chat.js';
import type { FunctionSchema } from '../../../tools/v2/core/types.js';

export interface ModelCallOptions {
  signal?: AbortSignal;
  tools?: Array<{ type: 'function'; function: FunctionSchema }>;
  tool_choice?: 'auto' | 'none' | 'required';
}

/**
 * è°ƒç”¨æœ¬åœ° Ollama æ¨¡å‹ï¼ˆV2 - æ”¯æŒ Function Callingï¼‰
 * 
 * æ³¨æ„ï¼šOllama æ”¯æŒ Function Callingï¼ˆä» 0.3.0+ ç‰ˆæœ¬å¼€å§‹ï¼‰
 * æ ¼å¼ä¸ OpenAI å…¼å®¹
 * 
 * @param messages - å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
 * @param options - è°ƒç”¨é€‰é¡¹ï¼ˆåŒ…å« tools å®šä¹‰ï¼‰
 */
export async function callLocalModelV2(
  messages: ChatMessage[],
  options: ModelCallOptions = {}
) {
  const fetch = (await import('node-fetch')).default;
  const modelName = process.env.OLLAMA_MODEL || 'deepseek-r1:7b';
  const ollamaUrl = process.env.OLLAMA_API_URL || 'http://localhost:11434';
  
  const requestBody: any = {
    model: modelName,
    messages,
    stream: true,
    keep_alive: '30m', // ä¿æŒæ¨¡å‹åœ¨å†…å­˜ä¸­ 30 åˆ†é’Ÿï¼Œé¿å…é¢‘ç¹é‡æ–°åŠ è½½
    options: {
      num_gpu: 999,  // å¼ºåˆ¶æ‰€æœ‰å±‚ä½¿ç”¨ GPUï¼ˆ999 è¡¨ç¤ºå°½å¯èƒ½å¤šï¼‰
    }
  };

  // âœ… å¦‚æœæä¾›äº†å·¥å…·å®šä¹‰ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
  if (options.tools && options.tools.length > 0) {
    requestBody.tools = options.tools;
    requestBody.tool_choice = options.tool_choice || 'auto';
    console.log(`ğŸ”§ [Ollama] ä¼ é€’ ${options.tools.length} ä¸ªå·¥å…·å®šä¹‰`);
  }
  
  const response = await fetch(`${ollamaUrl}/api/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(requestBody),
    signal: options.signal as any,
  });

  if (!response.ok) {
    throw new Error(`Ollama API é”™è¯¯: ${response.statusText}`);
  }

  return response.body;
}

/**
 * è°ƒç”¨ç«å±±å¼•æ“è±†åŒ…å¤§æ¨¡å‹ï¼ˆV2 - æ”¯æŒ Function Callingï¼‰
 * 
 * æ³¨æ„ï¼šç«å±±å¼•æ“è±†åŒ…æ”¯æŒ Function Calling
 * 
 * @param messages - å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
 * @param options - è°ƒç”¨é€‰é¡¹ï¼ˆåŒ…å« tools å®šä¹‰ï¼‰
 */
export async function callVolcengineModelV2(
  messages: ChatMessage[],
  options: ModelCallOptions = {}
) {
  // è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼ˆä¿æŒå…¼å®¹ï¼‰
  const volcengineMessages: VolcengineMessage[] = messages.map(msg => ({
    role: msg.role,
    content: msg.content,
  }));

  console.log('ğŸ”¥ è°ƒç”¨ç«å±±å¼•æ“è±†åŒ…æ¨¡å‹ï¼ˆV2 - Function Callingï¼‰...');
  
  const callOptions: any = {
    temperature: 0.7,
    maxTokens: 4000,
    topP: 0.95,
    signal: options.signal,
  };

  // âœ… å¦‚æœæä¾›äº†å·¥å…·å®šä¹‰ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
  if (options.tools && options.tools.length > 0) {
    // ç«å±±å¼•æ“çš„å·¥å…·æ ¼å¼ä¸ OpenAI å…¼å®¹
    callOptions.tools = options.tools;
    callOptions.tool_choice = options.tool_choice || 'auto';
    console.log(`ğŸ”§ [Volcengine] ä¼ é€’ ${options.tools.length} ä¸ªå·¥å…·å®šä¹‰`);
  }

  const stream = await volcengineService.chat(volcengineMessages, callOptions);

  return stream;
}

