/**
 * Process Long Text Analysis Use Case
 * 
 * å¤„ç†è¶…é•¿æ–‡æœ¬çš„Map-Reduceåˆ†æ
 * 
 * Map-Reduceæµç¨‹ï¼š
 * 1. Splitï¼šåˆ‡åˆ†è¶…é•¿æ–‡æœ¬ä¸ºå¤šä¸ªchunks
 * 2. Mapï¼šå¹¶è¡Œåˆ†ææ¯ä¸ªchunkï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯
 * 3. Reduceï¼šåˆå¹¶æ‰€æœ‰chunkçš„åˆ†æç»“æœ
 * 4. Finalï¼šç”Ÿæˆæœ€ç»ˆç»¼åˆè¯„å®¡æŠ¥å‘Š
 */

import { SSEStreamWriter } from '../../../../utils/sseStreamWriter.js';
import { splitTextIntoChunks, type TextChunk } from '../../../../utils/textChunker.js';
import { buildMapPrompt, buildReducePrompt } from '../../../../config/chunkingPrompts.js';
import { callVolcengineModel } from '../../../infrastructure/llm/llm-caller.js';
import { volcengineService } from '../../../infrastructure/llm/volcengine-service.js';
import { extractThinkingAndContent } from '../../../shared/utils/content-extractor.js';
import type { ChatMessage } from '../../../../types/chat.js';
import { getContainer } from '../../../di-container.js';

export interface ChunkingOptions {
  maxChunks?: number;
  includeCitations?: boolean;
}

export interface ExtractedData {
  goals: string[];
  milestones: string[];
  tasks: Array<{
    title: string;
    owner?: string;
    deadline?: string;
    dependsOn?: string;
  }>;
  metrics: string[];
  risks: Array<{
    risk: string;
    mitigation?: string;
  }>;
  unknowns: string[];
}

export class ProcessLongTextAnalysisUseCase {
  /**
   * æ‰§è¡Œè¶…é•¿æ–‡æœ¬åˆ†æ
   */
  async execute(
    message: string,
    userId: string,
    conversationId: string,
    clientAssistantMessageId: string | undefined,
    modelType: 'local' | 'volcano',
    sseWriter: SSEStreamWriter,
    options: ChunkingOptions = {}
  ): Promise<void> {
    console.log('ğŸ“¦ [Long Text Analysis] å¼€å§‹å¤„ç†è¶…é•¿æ–‡æœ¬...');
    
    try {
      // 1. Splitï¼šåˆ‡åˆ†æ–‡æœ¬
      const chunks = splitTextIntoChunks(message, {
        maxChunks: options.maxChunks || 30,
      });
      
      await sseWriter.sendEvent({
        type: 'chunking_init',
        totalChunks: chunks.length,
        estimatedSeconds: chunks.length * 5,
      });
      
      // 2. Mapï¼šåˆ†ææ¯ä¸ªchunk
      const extractedDataList: ExtractedData[] = [];
      
      for (let i = 0; i < chunks.length; i++) {
        const chunk = chunks[i];
        
        if (sseWriter.isClosed()) {
          console.log('âš ï¸  [Long Text Analysis] å®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œåœæ­¢å¤„ç†');
          return;
        }
        
        await sseWriter.sendEvent({
          type: 'chunking_progress',
          stage: 'map',
          chunkIndex: i,
          totalChunks: chunks.length,
        });
        
        console.log(`ğŸ” [Long Text Analysis] åˆ†æç¬¬ ${i + 1}/${chunks.length} æ®µ...`);
        
        const chunkData = await this.processChunk(chunk, i, chunks.length);
        extractedDataList.push(chunkData);
        
        await sseWriter.sendEvent({
          type: 'chunking_chunk',
          chunkIndex: i,
          chunkSummary: chunkData.goals.join('; '),
        });
      }
      
      // 3. Reduceï¼šåˆå¹¶æ•°æ®
      await sseWriter.sendEvent({
        type: 'chunking_progress',
        stage: 'reduce',
      });
      
      console.log('ğŸ”„ [Long Text Analysis] åˆå¹¶åˆ†æç»“æœ...');
      const mergedData = this.mergeExtractedData(extractedDataList);
      
      // 4. Finalï¼šç”Ÿæˆæœ€ç»ˆè¯„å®¡ï¼ˆæµå¼è¾“å‡ºï¼‰
      await sseWriter.sendEvent({
        type: 'chunking_progress',
        stage: 'final',
      });
      
      console.log('ğŸ“ [Long Text Analysis] ç”Ÿæˆæœ€ç»ˆè¯„å®¡æŠ¥å‘Š...');
      
      const accumulatedText = await this.generateFinalReport(
        mergedData,
        message,
        chunks.length,
        sseWriter
      );
      
      // 5. ä¿å­˜åˆ°æ•°æ®åº“
      if (accumulatedText) {
        await this.saveToDatabase(
          accumulatedText,
          conversationId,
          userId,
          clientAssistantMessageId,
          modelType
        );
      }
      
    } catch (error: any) {
      console.error('âŒ [Long Text Analysis] å¤„ç†å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * å¤„ç†å•ä¸ªchunkï¼ˆMapé˜¶æ®µï¼‰
   */
  private async processChunk(
    chunk: TextChunk,
    chunkIndex: number,
    totalChunks: number
  ): Promise<ExtractedData> {
    const prompt = buildMapPrompt(chunk.content, chunkIndex, totalChunks);
    const messages: ChatMessage[] = [
      { role: 'user', content: prompt }
    ];
    
    try {
      const stream = await callVolcengineModel(messages);
      
      let fullResponse = '';
      let buffer = '';
      
      for await (const streamChunk of stream) {
        const chunkStr = streamChunk.toString();
        buffer += chunkStr;
        
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';
        
        for (const line of lines) {
          if (line.trim() && !line.includes('[DONE]')) {
            const content = volcengineService.parseStreamLine(line);
            if (content) {
              fullResponse += content;
            }
          }
        }
      }
      
      return this.parseExtractedData(fullResponse, chunkIndex);
    } catch (error) {
      console.error(`âŒ [Long Text Analysis] Chunk ${chunkIndex} å¤„ç†å¤±è´¥:`, error);
      return this.getEmptyExtractedData();
    }
  }

  /**
   * è§£ææå–çš„æ•°æ®
   */
  private parseExtractedData(response: string, chunkIndex: number): ExtractedData {
    // å°è¯•ä»markdownä»£ç å—ä¸­æå–JSON
    const jsonMatch = response.match(/```json\s*([\s\S]*?)\s*```/);
    if (jsonMatch) {
      try {
        const jsonStr = jsonMatch[1];
        const parsed = JSON.parse(jsonStr);
        return this.normalizeExtractedData(parsed);
      } catch {
        // ç»§ç»­å°è¯•å…¶ä»–è§£ææ–¹æ³•
      }
    }
    
    // å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
    try {
      const parsed = JSON.parse(response);
      return this.normalizeExtractedData(parsed);
    } catch {
      console.warn(`âš ï¸  [Long Text Analysis] Chunk ${chunkIndex} JSON è§£æå¤±è´¥ï¼Œè¿”å›ç©ºæ•°æ®`);
      return this.getEmptyExtractedData();
    }
  }

  /**
   * è§„èŒƒåŒ–æå–çš„æ•°æ®
   */
  private normalizeExtractedData(parsed: any): ExtractedData {
    return {
      goals: parsed.extracted?.goals || [],
      milestones: parsed.extracted?.milestones || [],
      tasks: parsed.extracted?.tasks || [],
      metrics: parsed.extracted?.metrics || [],
      risks: parsed.extracted?.risks || [],
      unknowns: parsed.extracted?.unknowns || [],
    };
  }

  /**
   * è·å–ç©ºçš„æå–æ•°æ®
   */
  private getEmptyExtractedData(): ExtractedData {
    return {
      goals: [],
      milestones: [],
      tasks: [],
      metrics: [],
      risks: [],
      unknowns: [],
    };
  }

  /**
   * åˆå¹¶å¤šä¸ªchunkçš„æå–æ•°æ®ï¼ˆReduceé˜¶æ®µï¼‰
   */
  private mergeExtractedData(dataList: ExtractedData[]): ExtractedData {
    const merged: ExtractedData = this.getEmptyExtractedData();
    
    const normalize = (str: string) => str.trim().toLowerCase().replace(/\s+/g, ' ');
    
    const seenGoals = new Set<string>();
    const seenMilestones = new Set<string>();
    const seenMetrics = new Set<string>();
    const seenUnknowns = new Set<string>();
    const seenTaskTitles = new Set<string>();
    const seenRisks = new Set<string>();
    
    for (const data of dataList) {
      // åˆå¹¶goals
      for (const goal of data.goals) {
        const key = normalize(goal);
        if (!seenGoals.has(key)) {
          seenGoals.add(key);
          merged.goals.push(goal);
        }
      }
      
      // åˆå¹¶milestones
      for (const milestone of data.milestones) {
        const key = normalize(milestone);
        if (!seenMilestones.has(key)) {
          seenMilestones.add(key);
          merged.milestones.push(milestone);
        }
      }
      
      // åˆå¹¶tasks
      for (const task of data.tasks) {
        const key = normalize(task.title);
        if (!seenTaskTitles.has(key)) {
          seenTaskTitles.add(key);
          merged.tasks.push(task);
        }
      }
      
      // åˆå¹¶metrics
      for (const metric of data.metrics) {
        const key = normalize(metric);
        if (!seenMetrics.has(key)) {
          seenMetrics.add(key);
          merged.metrics.push(metric);
        }
      }
      
      // åˆå¹¶risks
      for (const risk of data.risks) {
        const key = normalize(risk.risk);
        if (!seenRisks.has(key)) {
          seenRisks.add(key);
          merged.risks.push(risk);
        }
      }
      
      // åˆå¹¶unknowns
      for (const unknown of data.unknowns) {
        const key = normalize(unknown);
        if (!seenUnknowns.has(key)) {
          seenUnknowns.add(key);
          merged.unknowns.push(unknown);
        }
      }
    }
    
    console.log(
      `âœ… [Long Text Analysis] åˆå¹¶å®Œæˆ: ${merged.goals.length} ç›®æ ‡, ` +
      `${merged.tasks.length} ä»»åŠ¡, ${merged.risks.length} é£é™©`
    );
    
    return merged;
  }

  /**
   * ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆFinalé˜¶æ®µï¼‰
   */
  private async generateFinalReport(
    mergedData: ExtractedData,
    originalMessage: string,
    totalChunks: number,
    sseWriter: SSEStreamWriter
  ): Promise<string> {
    const finalPrompt = buildReducePrompt(mergedData, originalMessage, totalChunks);
    const messages: ChatMessage[] = [
      { role: 'user', content: finalPrompt }
    ];
    
    const stream = await callVolcengineModel(messages);
    
    let buffer = '';
    let accumulatedText = '';
    
    for await (const chunk of stream) {
      if (sseWriter.isClosed()) break;
      
      const chunkStr = chunk.toString();
      buffer += chunkStr;
      
      const lines = buffer.split('\n');
      buffer = lines.pop() || '';
      
      for (const line of lines) {
        if (line.trim()) {
          const content = volcengineService.parseStreamLine(line);
          
          if (content) {
            accumulatedText += content;
            const { thinking, content: mainContent } = extractThinkingAndContent(accumulatedText);
            
            await sseWriter.sendEvent({
              content: mainContent,
              thinking: thinking || undefined,
            });
          }
          
          if (line.includes('[DONE]')) {
            console.log('âœ… [Long Text Analysis] æœ€ç»ˆè¯„å®¡å®Œæˆ');
            break;
          }
        }
      }
    }
    
    return accumulatedText;
  }

  /**
   * ä¿å­˜åˆ°æ•°æ®åº“
   */
  private async saveToDatabase(
    accumulatedText: string,
    conversationId: string,
    userId: string,
    clientAssistantMessageId: string | undefined,
    modelType: 'local' | 'volcano'
  ): Promise<void> {
    const { thinking, content } = extractThinkingAndContent(accumulatedText);
    
    const container = getContainer();
    const createMessageUseCase = container.getCreateMessageUseCase();
    const updateConversationUseCase = container.getUpdateConversationUseCase();
    
    await createMessageUseCase.execute(
      conversationId,
      userId,
      'assistant',
      content || accumulatedText,
      clientAssistantMessageId,
      modelType,
      thinking
    );
    
    const conversation = await container.getGetConversationUseCase().execute(conversationId, userId);
    if (conversation) {
      await updateConversationUseCase.execute(
        conversationId,
        userId,
        { messageCount: conversation.messageCount + 1 }
      );
    }
    
    console.log('âœ… [Long Text Analysis] æ¶ˆæ¯å·²ä¿å­˜åˆ°æ•°æ®åº“');
  }
}

