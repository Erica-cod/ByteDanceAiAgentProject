# 对话记忆管理功能说明

## 📚 功能概述

实现了 AI 模型的**对话上下文保持**功能，使模型能够"记住"之前的对话内容，提供更连贯的多轮对话体验。

---

## 🎯 实现方案

### **阶段 1: 滑动窗口 + 关键词匹配** ✅ 已实现

**核心特性：**
- ✅ 滑动窗口记忆：保留最近 N 轮对话
- ✅ Token 智能截断：防止超出模型上下文限制
- ✅ 关键词匹配增强：查找相关的历史对话
- ✅ 配置化设计：可根据场景调整参数

**工作原理：**
```
用户对话 1 → 保存到数据库
用户对话 2 → 保存到数据库
...
用户对话 15 → 保存到数据库

新消息到来时:
1. 从数据库读取最近 10 轮对话（20 条消息）
2. 搜索更早但相关的对话（基于关键词）
3. 合并并排序所有消息
4. 根据 Token 限制截断（保留最近的）
5. 发送给模型生成回复
```

---

### **阶段 2: 向量检索记忆** 🚧 计划中（明天实现）

**计划特性：**
- 🔮 语义相似度检索：使用 Embedding 向量
- 🔮 长期记忆管理：可处理超长对话历史
- 🔮 混合检索策略：结合关键词和语义
- 🔮 跨对话检索：查找用户所有对话中的相关信息

**技术栈：**
- Ollama Embeddings (nomic-embed-text)
- FAISS 本地向量存储
- LangChain.js 向量检索

---

## 📖 使用说明

### **1. 基本使用**

功能已自动集成，无需额外配置即可使用默认设置：

```typescript
// 默认配置
{
  windowSize: 10,           // 保留最近 10 轮对话
  maxTokens: 4000,          // 最大 4000 tokens
  enableKeywordMatch: true, // 启用关键词匹配
  keywordMatchCount: 3      // 额外检索 3 条相关消息
}
```

### **2. 自定义配置**

#### **方法 1：环境变量配置**

在 `.env.local` 或 `.env.production` 中添加：

```bash
# 滑动窗口大小（保留最近几轮对话）
MEMORY_WINDOW_SIZE=15

# 最大 Token 限制
MEMORY_MAX_TOKENS=6000

# 是否启用关键词匹配
MEMORY_ENABLE_KEYWORD_MATCH=true
```

#### **方法 2：代码配置**

修改 `api/config/memoryConfig.ts` 中的 `DEFAULT_MEMORY_CONFIG`：

```typescript
export const DEFAULT_MEMORY_CONFIG: MemoryConfig = {
  windowSize: 15,      // 改为 15 轮
  maxTokens: 6000,     // 改为 6000 tokens
  enableKeywordMatch: true,
  keywordMatchCount: 5,
};
```

---

## 🔧 配置参数说明

### **windowSize - 滑动窗口大小**

含义：保留最近几轮对话（1 轮 = 1 条用户消息 + 1 条助手回复）

| 值 | 适用场景 | 说明 |
|---|---|---|
| 5-8 | 简单问答、快速对话 | 上下文较少，响应快 |
| 10-15 | 标准对话（推荐） | 平衡性能和上下文 |
| 20+ | 复杂任务、长对话 | 需要更多历史信息 |

**示例：**
```
windowSize = 3 时：
用户: "我喜欢披萨"
助手: "很好！"
用户: "推荐一家餐厅"
助手: "市中心有一家..."
用户: "营业时间？"      ← 当前消息
助手: "..."             ← 模型能看到最近 3 轮共 6 条消息
```

### **maxTokens - Token 限制**

含义：发送给模型的最大 token 数（约 1 token = 3-4 个字符）

| 值 | 适用模型 | 说明 |
|---|---|---|
| 2000-4000 | 一般模型 | 标准配置 |
| 6000-8000 | 长上下文模型 | DeepSeek-R1, GPT-4 等 |
| 16000+ | 超长上下文模型 | Claude, GPT-4 Turbo 等 |

**注意：** 确保不超过模型的上下文窗口限制！

### **enableKeywordMatch - 关键词匹配**

含义：是否搜索更早但相关的对话

- `true`：启用（推荐）- 可能找到相关历史
- `false`：禁用 - 只使用最近消息，速度更快

**示例：**
```
对话历史：
1. "我的名字是张三" (20轮前)
2. ...
10. "今天天气真好" (最近)
11. "我想出去玩" (最近)

用户问："我叫什么名字？"

启用关键词匹配：会找到第1条消息 ✅
禁用：只看最近10轮，找不到 ❌
```

---

## 🧪 测试验证

### **测试场景 1：基础上下文保持**

```
1. 用户: "我喜欢吃披萨"
   助手: "很好！披萨是一种很受欢迎的食物。"

2. 用户: "推荐一家好吃的店"
   助手: "市中心有家XX披萨店，口碑很好。"   ✅ 记住了"披萨"

3. 用户: "他们的营业时间是？"
   助手: "XX披萨店的营业时间是..."          ✅ 记住了"那家店"
```

### **测试场景 2：长对话测试**

```
进行 15+ 轮对话，验证：
- ✅ 最近的对话始终被记住
- ✅ Token 超限时正确截断
- ✅ 对话流畅不中断
```

### **测试场景 3：关键词检索**

```
1. 用户: "我的邮箱是 test@example.com" (10轮前)
2-10. [其他无关对话]
11. 用户: "我的邮箱是什么？"
    助手: "您的邮箱是 test@example.com"   ✅ 通过关键词找到了
```

---

## 📊 性能监控

在控制台日志中可以看到：

```
🧠 ConversationMemoryService - 开始构建对话上下文
📊 配置: 窗口大小=10, Token限制=4000
✅ 获取到 18 条最近消息
🔍 通过关键词匹配找到 2 条相关历史消息
📝 最终上下文包含 22 条消息
📊 预估 token 数: 3245
```

---

## 🔄 阶段对比

### **阶段 0（之前）- 无记忆**
```typescript
❌ 问题：
const messages = [
  { role: 'system', content: SYSTEM_PROMPT },
  { role: 'user', content: currentMessage }  // 只有当前消息
];

用户: "我喜欢披萨"
助手: "很好！"
用户: "推荐一家店"
助手: "你想找什么类型的店？"  ❌ 不知道用户喜欢披萨
```

### **阶段 1（当前）- 滑动窗口**
```typescript
✅ 改进：
const messages = await memoryService.getConversationContext(
  conversationId, userId, currentMessage, SYSTEM_PROMPT
);
// 包含最近 10 轮对话 + 相关历史

用户: "我喜欢披萨"
助手: "很好！"
用户: "推荐一家店"
助手: "基于您喜欢披萨，推荐XX店"  ✅ 记住了上下文
```

### **阶段 2（明天）- 向量检索**
```typescript
🔮 未来：
// 将使用语义相似度，不只是关键词
// 可以处理更长的对话历史
// 检索更精准
```

---

## 📝 代码位置

### **核心文件：**
- `api/services/conversationMemoryService.ts` - 记忆服务实现
- `api/config/memoryConfig.ts` - 配置管理
- `api/lambda/chat.ts` - 集成点（790-850 行）

### **配置文件：**
- `.env.example` - 环境变量模板
- `.env.production` - 生产环境配置

---

## 🐛 故障排查

### **问题：模型还是不记得之前的对话**

**检查：**
1. 确认 conversationId 正确传递
2. 查看控制台日志确认消息数量
3. 检查数据库中是否保存了消息

```bash
# 检查数据库
npm run db:shell
> db.messages.find({ conversationId: "xxx" }).count()
```

### **问题：响应变慢**

**原因：** 可能窗口太大或关键词匹配开销

**解决：**
```bash
# 减小窗口
MEMORY_WINDOW_SIZE=5

# 或禁用关键词匹配
MEMORY_ENABLE_KEYWORD_MATCH=false
```

### **问题：Token 超限错误**

**原因：** maxTokens 设置超过模型限制

**解决：**
```bash
# 降低限制
MEMORY_MAX_TOKENS=2000
```

---

## 🎉 效果展示

使用前后对比：

| 特性 | 阶段 0（无记忆） | 阶段 1（滑动窗口） |
|---|---|---|
| 记住用户偏好 | ❌ | ✅ |
| 连续对话 | ❌ | ✅ |
| 代词理解 | ❌ | ✅ |
| 历史查询 | ❌ | ✅（关键词） |
| 长对话支持 | ❌ | ✅（10轮+） |

---

## 🚀 下一步

明天将实现**阶段 2：向量检索记忆**，带来：
- 🔮 语义理解（不只是关键词）
- 🔮 更长的记忆（100+ 轮对话）
- 🔮 更精准的检索
- 🔮 跨对话记忆

敬请期待！

