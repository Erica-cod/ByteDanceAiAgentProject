# 服务端 / 框架侧：面试追问与标准回答（贴合本项目）

## 1) 为什么选 Modern.js（而不是自己搭 Express / Next / 传统分离式后端）？

- **一句话回答**：我需要一个“前后端一体的 BFF”，既能快速提供 API（含 SSE 流式），又能减少跨域/部署复杂度，所以选了 Modern.js 的 BFF 插件。
- **展开说法（面试话术）**
  - **BFF 同源优势**：前端用相对路径 `/api/*` 调用，默认没有 CORS 问题，开发/部署更省心。
  - **约定式路由**：`api/lambda/*` 自动变成路由，适合快速扩展端点。
  - **工程化**：Modern.js 自带构建/开发服务器流程（底层是 Rust 构建体系），日常体验更顺滑。
  - **项目约束**：本项目是 TypeScript + ESM，部分 DI 框架在 ESM 环境兼容性一般，所以我在后端 DI 上采用了更轻量的工厂/容器方式（降低框架耦合）。
- **项目证据**
  - `modern.config.ts`：启用 `@modern-js/plugin-bff`，BFF 前缀 `/api`，开发端口 8080
  - `CORS_SETUP_COMPLETE.md`：解释 BFF 同源天然避免 CORS，并说明 Modern.js 的优势

## 2) 为什么用 SSE，不用 WebSocket？

- **一句话回答**：AI 输出是“服务端 → 客户端”的单向流式，SSE 更贴合场景、部署更简单、穿透代理更稳。
- **对比点**
  - **方向**：SSE 单向足够；WebSocket 双工是能力过剩。
  - **部署/代理**：SSE 基于 HTTP，更容易过 CDN/反向代理/企业网关；WebSocket 更容易被环境限制。
  - **重连**：SSE/HTTP 流更容易做“断点续传 + 重新请求”；WebSocket 需要更复杂的状态机。
- **项目证据**
  - `docs/03-Streaming/README.md`：SSE vs WebSocket 的面试要点
  - `api/lambda/chat.ts`：`TransformStream` + `text/event-stream` 返回 SSE

## 3) 为什么前端用 `fetch + ReadableStream`，不用 `EventSource`？

- **一句话回答**：因为我需要 **POST body、队列 token、上传会话参数、AbortController 取消**，这些 `EventSource` 天然不支持或很别扭。
- **展开说法**
  - `EventSource` 只能 GET，不能方便地带 JSON body（本项目 `/api/chat` 是 POST，参数很多：`mode/modelType/deviceId/queueToken/uploadSessionId/...`）。
  - `fetch` 读取 `response.body.getReader()` 可以完全自定义协议解析、重试策略和取消逻辑。
- **项目证据**
  - `src/hooks/data/useSSEStream.ts`：POST `/api/chat` + `ReadableStream` 解析 `data:` 行；支持 429 队列、指数退避重连、AbortController

## 4) 打字机效果“节奏”怎么控制？逐字还是分块？

- **一句话回答**：后端按上游模型的 stream chunk 实时推送（本质是“分块”），前端做渲染侧的节流/虚拟化，避免每次更新都触发布局抖动。
- **展开说法**
  - **服务端**：收到模型增量就立刻 `data: { content, thinking }` 推送（用户体感最“跟手”）。
  - **续流场景**：为了模拟打字机，续传会按固定 chunkSize 切片渐进发送（可控节奏）。
  - **前端**：不对每个 chunk 都做重排；只在内容变化到一定阈值后才重新测量最后一条消息高度，降低卡顿。
- **项目证据**
  - `api/handlers/sseHandler.ts`：增量解析并频繁 `safeWrite` 推送
  - `api/handlers/resumeHandler.ts`：续流时用 `chunkSize=10` 模拟打字机
  - `src/components/old-structure/MessageList.tsx`：流式阶段高度重算做阈值 + 防抖（减少 CLS/重排）

## 5) SSE 的心跳/超时怎么做？有没有参考开源方案？

- **一句话回答**：服务端定时发送 `: keep-alive` 注释行，避免反向代理空闲超时；客户端做重连与退避；这是 SSE 的经典生产实践。
- **项目证据**
  - `api/handlers/sseHandler.ts`：`HEARTBEAT_MS` 默认 15s，`setInterval(() => safeWrite(': keep-alive'))`
  - `api/handlers/sseStreamWriter.ts`：抽象了心跳定时器
  - `docs/03-Streaming/README.md`：生产环境问题里专门提到“心跳避免空闲超时”

## 6) 断线重连策略是什么？怎么避免“惊群重试”？

- **一句话回答**：客户端最多 3 次重连，指数退避 + jitter；如果是 429 排队，则按 `Retry-After` 重试并携带队列 token 保持位置。
- **项目证据**
  - `src/hooks/data/useSSEStream.ts`：`MAX_RECONNECT_ATTEMPTS=3`，`computeBackoff()` 指数退避 + 随机抖动
  - `api/_clean/infrastructure/streaming/sse-limiter.ts`：返回 `Retry-After`、`X-Queue-Token`、`X-Queue-Position`、`X-Queue-Estimated-Wait`
  - `api/_clean/infrastructure/queue/queue-manager.ts`：为 `Retry-After` 增加 jitter，并对无效 token 做冷却惩罚（防刷）

## 7) 限流/并发控制怎么做？为什么不用 Redis 计数？

- **一句话回答**：SSE 限流保护的是**单台服务的本地资源**，内存计数最快、零依赖；Redis 只有在“单地区多实例负载均衡且需要全局精确并发”时才必要。
- **项目证据**
  - `api/_clean/infrastructure/streaming/sse-limiter.ts`：`activeGlobal` + `activeByUser`（默认全局 200、单用户 1），满了就入队并返回 token
  - `docs/00-Project-Overview/ARCHITECTURE_DECISION.md`：用数据规模分析解释为什么选择内存而不是 Redis

## 8) Redis 在项目里到底用不用？为什么？

- **一句话回答**：Redis 是“可选增强”：用于多 Agent 模式的断点续传/状态缓存（节省 token 成本），但不是 SSE 限流的默认依赖。
- **展开说法**
  - **限流**：本地资源保护 → 内存最合适（快、可靠、无运维）。
  - **多 Agent 状态**：一旦中断重跑会浪费大量 token → 用 Redis 做短 TTL 状态缓存更划算。
- **项目证据**
  - `docs/08-Data-Management/REDIS_SETUP.md`：多 Agent 断点续传的 Redis 设计（key/TTL/恢复流程）
  - `api/lambda/chat.ts`：多 Agent 调用支持 `resumeFromRound`

## 9) 断点续传怎么做？怎么区分“用户主动停止”和“网络波动”？

- **一句话回答**：多 Agent 用“轮次级别”的续传（`resumeFromRound`）；单 Agent 预留了“字符偏移级别”的续流接口（`resumeFrom: {messageId, position}`），并在服务端实现了续流读取与分段发送。
- **项目证据**
  - **单 Agent（预留/已实现服务端）**：`api/handlers/resumeHandler.ts` 支持 `resumeFrom`，从进度仓库取 `accumulatedText`，从 position 继续发
  - **多 Agent（已落地前端/后端协议）**：`src/hooks/data/useSSEStream.ts` 会在重试时携带 `resumeFromRound`；后端 `api/lambda/chat.ts` 接收并交给 `handleMultiAgentMode`
  - **用户意图区分（设计依据）**：`docs/03-Streaming/STREAM_RESUME_USER_INTENT.md`
- **补充（面试更稳的说法）**
  - 用户点击“停止”本质是客户端主动 `AbortController.abort()`；网络断开则触发 fetch 读取异常/连接中断，两者错误类型不同，可分别处理。
  - 服务端通过 `safeWrite` 捕获 `AbortError/ERR_STREAM_PREMATURE_CLOSE`，避免把“客户端断连”当成异常刷屏。

## 10) 如何避免连接泄漏/僵尸连接？

- **一句话回答**：服务端写入统一走 `safeWrite`，一旦检测到客户端断开就标记关闭并停止后续写入；同时在 finally 里释放 SSE 并发名额，避免泄漏。
- **项目证据**
  - `api/handlers/sseHandler.ts`：`isStreamClosed` + `safeWrite()` 捕获 AbortError；finally 里 `onFinally?.()` 释放名额
  - `api/lambda/chat.ts`：`handoffToStream` 机制，确保“没进入流式就立即释放名额”

## 11) 大量用户同时流式，会不会打爆内存？

- **一句话回答**：服务端用并发上限 + 队列化保护；前端用虚拟列表 + 渐进式加载大消息；同时对超长文本采用 chunking/分段策略，避免单次渲染/传输过大。
- **项目证据**
  - `api/_clean/infrastructure/streaming/sse-limiter.ts`、`api/_clean/infrastructure/queue/queue-manager.ts`：并发保护 + 排队
  - `docs/05-Large-Text-Handling/COMPLETE_LARGE_TEXT_SOLUTION.md`：大文本上传/存储/展示全链路方案
  - `api/lambda/chat.ts`：超长文本触发 chunking 分段处理（`message.length > 12000` 等）

## 12) AI 返回的 Markdown/JSON 格式可能截断或错误，怎么做容错？

- **一句话回答**：分两层容错：**JSON 用括号平衡算法提取完整对象并忽略尾部垃圾**；**Markdown 用自动修复器检测并补全不完整标记（代码块、表格、HTML标签等），并提供备用渲染器作为降级方案**。
- **项目证据**
  - **JSON 容错**（服务端）
    - `api/_clean/shared/utils/json-extractor.ts`：`extractJSON()` 使用括号平衡算法，智能截取第一个完整 JSON，忽略后面的垃圾字符
    - `docs/12-Miscellaneous/JSON_GARBAGE_FIX.md`：AI 在长文本生成时可能在 JSON 后多生成 `"}` 等垃圾字符，用栈算法解决
  - **Markdown 容错**（前端）
    - `src/utils/markdownFixer.ts`：`fixIncompleteMarkdown()` 自动修复未闭合的代码块、HTML 标签、链接、表格等
    - `src/utils/fallbackMarkdownRenderer.tsx`：手工实现的 Markdown 解析器，当 react-markdown 失败时自动降级
    - `src/components/business/Message/StreamingMarkdown.tsx`：整合三层容错（自动修复 → react-markdown → 备用渲染器）
  - **设计思路**：只在检测到流式传输特征时才应用修复，避免误伤正常内容；react-markdown 失败时不会白屏，自动切换到备用渲染器

## 13) 安全：有没有防 XSS / 防中间人攻击？

- **一句话回答**：传输层依赖 HTTPS；展示层不渲染 raw HTML（默认防 XSS）；链接打开加 `noopener noreferrer`；无登录体系用设备指纹 + 本地加密缓存保护隐私数据。
- **项目证据**
  - `src/components/business/Message/StreamingMarkdown.tsx`：使用 `react-markdown`，未启用 `rehype-raw`；自定义 `a` 标签加 `rel="noopener noreferrer"`
  - `docs/02-Security-System/README.md`：无登录安全系统（设备指纹 + 加密存储 + 数据隔离）
  - `docs/02-Security-System/CORS_CONFIGURATION.md`、`CORS_SETUP_COMPLETE.md`：BFF 同源默认无跨域，分离部署才需要配置 CORS 白名单

## 14) 为什么不用微软的 fetch EventSource polyfill / 现成 SSE SDK？

- **一句话回答**：我这里不是"GET-only 的 EventSource 场景"，而是"POST + 复杂参数 + 上传会话 + 自定义重试/队列协议"，自己用 fetch 解析流最可控、也最贴合业务。


