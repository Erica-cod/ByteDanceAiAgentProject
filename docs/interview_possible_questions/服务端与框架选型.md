# 服务端 / 框架侧：面试追问与标准回答（贴合本项目）

## 1) 为什么选 Modern.js（而不是自己搭 Express / Next / 传统分离式后端）？

- **一句话回答**：我需要一个“前后端一体的 BFF”，既能快速提供 API（含 SSE 流式），又能减少跨域/部署复杂度，所以选了 Modern.js 的 BFF 插件。
- **展开说法（面试话术）**
  - **BFF 同源优势**：前端用相对路径 `/api/*` 调用，默认没有 CORS 问题，开发/部署更省心。
  - **约定式路由**：`api/lambda/*` 自动变成路由，适合快速扩展端点。
  - **工程化**：Modern.js 自带构建/开发服务器流程（底层是 Rust 构建体系），日常体验更顺滑。
  - **项目约束**：本项目是 TypeScript + ESM，部分 DI 框架在 ESM 环境兼容性一般，所以我在后端 DI 上采用了更轻量的工厂/容器方式（降低框架耦合）。
- **项目证据**
  - `modern.config.ts`：启用 `@modern-js/plugin-bff`，BFF 前缀 `/api`，开发端口 8080
  - `CORS_SETUP_COMPLETE.md`：解释 BFF 同源天然避免 CORS，并说明 Modern.js 的优势

## 2) 为什么用 SSE，不用 WebSocket？

- **一句话回答**：AI 输出是“服务端 → 客户端”的单向流式，SSE 更贴合场景、部署更简单、穿透代理更稳。
- **对比点**
  - **方向**：SSE 单向足够；WebSocket 双工是能力过剩。
  - **部署/代理**：SSE 基于 HTTP，更容易过 CDN/反向代理/企业网关；WebSocket 更容易被环境限制。
  - **重连**：SSE/HTTP 流更容易做“断点续传 + 重新请求”；WebSocket 需要更复杂的状态机。
- **项目证据**
  - `docs/03-Streaming/README.md`：SSE vs WebSocket 的面试要点
  - `api/lambda/chat.ts`：`TransformStream` + `text/event-stream` 返回 SSE

## 3) 为什么前端用 `fetch + ReadableStream`，不用 `EventSource`？

- **一句话回答**：因为我需要 **POST body、队列 token、上传会话参数、AbortController 取消**，这些 `EventSource` 天然不支持或很别扭。
- **展开说法**
  - `EventSource` 只能 GET，不能方便地带 JSON body（本项目 `/api/chat` 是 POST，参数很多：`mode/modelType/deviceId/queueToken/uploadSessionId/...`）。
  - `fetch` 读取 `response.body.getReader()` 可以完全自定义协议解析、重试策略和取消逻辑。
- **项目证据**
  - `src/hooks/data/useSSEStream.ts`：POST `/api/chat` + `ReadableStream` 解析 `data:` 行；支持 429 队列、指数退避重连、AbortController

## 4) 打字机效果“节奏”怎么控制？逐字还是分块？

- **一句话回答**：后端按上游模型的 stream chunk 实时推送（本质是“分块”），前端做渲染侧的节流/虚拟化，避免每次更新都触发布局抖动。
- **展开说法**
  - **服务端**：收到模型增量就立刻 `data: { content, thinking }` 推送（用户体感最“跟手”）。
  - **续流场景**：为了模拟打字机，续传会按固定 chunkSize 切片渐进发送（可控节奏）。
  - **前端**：不对每个 chunk 都做重排；只在内容变化到一定阈值后才重新测量最后一条消息高度，降低卡顿。
- **项目证据**
  - `api/handlers/sseHandler.ts`：增量解析并频繁 `safeWrite` 推送
  - `api/handlers/resumeHandler.ts`：续流时用 `chunkSize=10` 模拟打字机
  - `src/components/old-structure/MessageList.tsx`：流式阶段高度重算做阈值 + 防抖（减少 CLS/重排）

## 5) SSE 的心跳/超时怎么做？有没有参考开源方案？

- **一句话回答**：服务端定时发送 `: keep-alive` 注释行，避免反向代理空闲超时；客户端做重连与退避；这是 SSE 的经典生产实践。
- **项目证据**
  - `api/handlers/sseHandler.ts`：`HEARTBEAT_MS` 默认 15s，`setInterval(() => safeWrite(': keep-alive'))`
  - `api/handlers/sseStreamWriter.ts`：抽象了心跳定时器
  - `docs/03-Streaming/README.md`：生产环境问题里专门提到“心跳避免空闲超时”

## 6) 断线重连策略是什么？怎么避免“惊群重试”？

- **一句话回答**：客户端最多 3 次重连，指数退避 + jitter；如果是 429 排队，则按 `Retry-After` 重试并携带队列 token 保持位置。
- **项目证据**
  - `src/hooks/data/useSSEStream.ts`：`MAX_RECONNECT_ATTEMPTS=3`，`computeBackoff()` 指数退避 + 随机抖动
  - `api/_clean/infrastructure/streaming/sse-limiter.ts`：返回 `Retry-After`、`X-Queue-Token`、`X-Queue-Position`、`X-Queue-Estimated-Wait`
  - `api/_clean/infrastructure/queue/queue-manager.ts`：为 `Retry-After` 增加 jitter，并对无效 token 做冷却惩罚（防刷）

## 7) 限流/并发控制怎么做？为什么不用 Redis 计数？

- **一句话回答**：SSE 限流保护的是**单台服务的本地资源**，内存计数最快、零依赖；Redis 只有在“单地区多实例负载均衡且需要全局精确并发”时才必要。
- **项目证据**
  - `api/_clean/infrastructure/streaming/sse-limiter.ts`：`activeGlobal` + `activeByUser`（默认全局 200、单用户 1），满了就入队并返回 token
  - `docs/00-Project-Overview/ARCHITECTURE_DECISION.md`：用数据规模分析解释为什么选择内存而不是 Redis

## 8) Redis 在项目里到底用不用？为什么？

- **一句话回答**：Redis 是“可选增强”：用于多 Agent 模式的断点续传/状态缓存（节省 token 成本），但不是 SSE 限流的默认依赖。
- **展开说法**
  - **限流**：本地资源保护 → 内存最合适（快、可靠、无运维）。
  - **多 Agent 状态**：一旦中断重跑会浪费大量 token → 用 Redis 做短 TTL 状态缓存更划算。
- **项目证据**
  - `docs/08-Data-Management/REDIS_SETUP.md`：多 Agent 断点续传的 Redis 设计（key/TTL/恢复流程）
  - `api/lambda/chat.ts`：多 Agent 调用支持 `resumeFromRound`

## 9) 断点续传怎么做？怎么区分“用户主动停止”和“网络波动”？

- **一句话回答**：多 Agent 用“轮次级别”的续传（`resumeFromRound`）；单 Agent 预留了“字符偏移级别”的续流接口（`resumeFrom: {messageId, position}`），并在服务端实现了续流读取与分段发送。
- **项目证据**
  - **单 Agent（预留/已实现服务端）**：`api/handlers/resumeHandler.ts` 支持 `resumeFrom`，从进度仓库取 `accumulatedText`，从 position 继续发
  - **多 Agent（已落地前端/后端协议）**：`src/hooks/data/useSSEStream.ts` 会在重试时携带 `resumeFromRound`；后端 `api/lambda/chat.ts` 接收并交给 `handleMultiAgentMode`
  - **用户意图区分（设计依据）**：`docs/03-Streaming/STREAM_RESUME_USER_INTENT.md`
- **补充（面试更稳的说法）**
  - 用户点击“停止”本质是客户端主动 `AbortController.abort()`；网络断开则触发 fetch 读取异常/连接中断，两者错误类型不同，可分别处理。
  - 服务端通过 `safeWrite` 捕获 `AbortError/ERR_STREAM_PREMATURE_CLOSE`，避免把“客户端断连”当成异常刷屏。

## 10) 如何避免连接泄漏/僵尸连接？

- **一句话回答**：服务端写入统一走 `safeWrite`，一旦检测到客户端断开就标记关闭并停止后续写入；同时在 finally 里释放 SSE 并发名额，避免泄漏。
- **项目证据**
  - `api/handlers/sseHandler.ts`：`isStreamClosed` + `safeWrite()` 捕获 AbortError；finally 里 `onFinally?.()` 释放名额
  - `api/lambda/chat.ts`：`handoffToStream` 机制，确保“没进入流式就立即释放名额”

## 11) 大量用户同时流式，会不会打爆内存？

- **一句话回答**：服务端用并发上限 + 队列化保护；前端用虚拟列表 + 渐进式加载大消息；同时对超长文本采用 chunking/分段策略，避免单次渲染/传输过大。
- **项目证据**
  - `api/_clean/infrastructure/streaming/sse-limiter.ts`、`api/_clean/infrastructure/queue/queue-manager.ts`：并发保护 + 排队
  - `docs/05-Large-Text-Handling/COMPLETE_LARGE_TEXT_SOLUTION.md`：大文本上传/存储/展示全链路方案
  - `api/lambda/chat.ts`：超长文本触发 chunking 分段处理（`message.length > 12000` 等）

## 12) AI 返回的 Markdown/JSON 格式可能截断或错误，怎么做容错？

- **一句话回答**：分两层容错：**JSON 用括号平衡算法提取完整对象并忽略尾部垃圾**；**Markdown 用自动修复器检测并补全不完整标记（代码块、表格、HTML标签等），并提供备用渲染器作为降级方案**。
- **项目证据**
  - **JSON 容错**（服务端）
    - `api/_clean/shared/utils/json-extractor.ts`：`extractJSON()` 使用括号平衡算法，智能截取第一个完整 JSON，忽略后面的垃圾字符
    - `docs/12-Miscellaneous/JSON_GARBAGE_FIX.md`：AI 在长文本生成时可能在 JSON 后多生成 `"}` 等垃圾字符，用栈算法解决
  - **Markdown 容错**（前端）
    - `src/utils/markdownFixer.ts`：`fixIncompleteMarkdown()` 自动修复未闭合的代码块、HTML 标签、链接、表格等
    - `src/utils/fallbackMarkdownRenderer.tsx`：手工实现的 Markdown 解析器，当 react-markdown 失败时自动降级
    - `src/components/business/Message/StreamingMarkdown.tsx`：整合三层容错（自动修复 → react-markdown → 备用渲染器）
  - **设计思路**：只在检测到流式传输特征时才应用修复，避免误伤正常内容；react-markdown 失败时不会白屏，自动切换到备用渲染器

## 13) 安全：有没有防 XSS / 防中间人攻击？

- **一句话回答**：传输层依赖 HTTPS；展示层不渲染 raw HTML（默认防 XSS）；链接打开加 `noopener noreferrer`；无登录体系用设备指纹 + 本地加密缓存保护隐私数据。
- **项目证据**
  - `src/components/business/Message/StreamingMarkdown.tsx`：使用 `react-markdown`，未启用 `rehype-raw`；自定义 `a` 标签加 `rel="noopener noreferrer"`
  - `docs/02-Security-System/README.md`：无登录安全系统（设备指纹 + 加密存储 + 数据隔离）
  - `docs/02-Security-System/CORS_CONFIGURATION.md`、`CORS_SETUP_COMPLETE.md`：BFF 同源默认无跨域，分离部署才需要配置 CORS 白名单

## 14) 为什么不用微软的 fetch EventSource polyfill / 现成 SSE SDK？

- **一句话回答**：我这里不是"GET-only 的 EventSource 场景"，而是"POST + 复杂参数 + 上传会话 + 自定义重试/队列协议"，自己用 fetch 解析流最可控、也最贴合业务。


## 15) 中断请求怎么做的
- **一句话回答**：使用了abortController注入singal,abortController和cancelToken一样都是基于观察者模式，发起请求时注入，相当于订阅取消函数，abort时执行。但是fetch只支持abortController，而且cancelToken只支持取消功能，而abortController可以查看取消状态，而且我项目中有大文件分片上传，cancelToken的使用会出现闭包的缺点内存泄露：当多个request无中断发生结束时，每一个request仍然被取消oncancel引用并且持有文件分片，根据JS垃圾回收采用的标记清除算法，从根节点开始染色可达的变量，没有染色的被清除。所以一直不会被清除。现在官方也不推荐cancelToken。


## 16) 印象最深刻bug
### 面试回答模板：我解决过印象最深的 Bug（Modern.js + 装饰器 + 路由扫描）

#### 背景（Situation）
我在重构后端架构，从旧的 `services` 逐步迁移到 Clean Architecture。项目用的是 Modern.js 的 BFF（`api/` 相关目录会参与扫描/构建）。为了更像 Nest/Spring，我一开始引入了装饰器 DI（尤其是参数装饰器 `@Inject('token')`）。

#### 问题（Task）
出现一个很“诡异”的问题：**明明把新架构代码放在以下划线开头的目录里（例如 `_clean`），按约定应该不会作为路由，但 Modern.js 仍会把里面的文件当成路由相关文件去处理，最终导致编译/解析失败**。更麻烦的是：
- 官方文档只讲“下划线不会生成路由”，但没讲“扫描/解析阶段仍可能读取这些文件”；
- GitHub/社区几乎搜不到同款问题，短期内没有可抄答案。

#### 排查思路（Action）
我当时做了三步：

- **先验证“不是业务代码错了”，而是框架扫描链路问题**  
  我把同一段逻辑换成不带装饰器的普通类，问题就消失；一加回 `@Inject('token')` 又报错，说明核心在“装饰器语法/元数据”进入了扫描解析链路。

- **退而求其次：先保证迁移能推进（工程取舍）**  
  我没有死磕参数装饰器，而是改成更“朴素可控”的方式：  
  - 用 `di-container.ts` 做显式依赖装配（工厂/容器模式），避免扫描时触发复杂装饰器解析；  
  - **延迟初始化**（第一次请求/真正用到时才创建容器/实例），减少被框架启动扫描阶段提前触发的概率；  
  - 用“分模块迁移”把风险隔离：一个模块迁完验证稳定再迁下一个，避免大爆炸式重构。

- **最后再回头把“注解体验”补回来（最终解）**  
  等整体迁移稳定后，我再解决装饰器问题本身：  
  - **把易出问题的参数装饰器改为类装饰器**（例如 `@Inject(['IMetricsRepository'])` 这种声明依赖数组的方式），兼容性明显更好；  
  - 明确开启 `experimentalDecorators` / `emitDecoratorMetadata`，让装饰器在构建链路里被正确处理；  
  - 把装饰器容器和示例（如 `decorator-di-example.ts`）跑通，证明方案可复制。

#### 结果（Result）
- 重构没有被一个框架扫描问题卡死：我用容器 + 延迟初始化把迁移路线跑通，实现了**分模块平滑替换旧架构**；
- 最终也实现了“声明式 DI”的体验：装饰器方案可用，且更稳（类装饰器替代参数装饰器）；
- 这次让我在工程上更有把握：**先交付可运行、可迭代的架构，再逐步把“优雅性/体验”补齐**。

#### 反思与收获（Takeaway）
- 文档没写不代表不存在：很多框架的“忽略规则”只影响最终产物（比如路由生成），不一定影响前置的扫描/解析阶段；
- 遇到难以定位且外部资料稀缺的问题，最有效的是：**缩小变量、构造最小复现、先用工程手段绕开风险点保证进度，再逐步回归理想方案**；
- 装饰器这类“语法级特性”要特别注意：**不同解析器/构建链路对参数装饰器的兼容性差异很大**，类装饰器往往更稳。


### 前端岗位被问“用过注解/装饰器吗”——推荐回答（30–60 秒）

- **我用过 TypeScript 装饰器**。在业务里我主要用它做“声明式能力”：比如依赖注入（DI）、缓存/埋点/鉴权这类横切逻辑，写法上接近 NestJS 的 `@Injectable()` / `@Controller()` 思路。  
- **原理我理解**：装饰器本质是一个函数，会在类定义阶段被调用；我会把需要的配置（比如 token、作用域、拦截规则）写到类的元数据里，运行时再由容器/中间件读取元数据去执行对应逻辑。  
- **工程层面我也踩过坑**：装饰器属于 TS 的实验特性，依赖构建链配置一致（`experimentalDecorators`，以及需要元数据时的 `emitDecoratorMetadata` + `reflect-metadata`）。我最终选了兼容性更好的“类装饰器 + 显式 token”方案。

---

### 前端视角的“价值点”（面试官更爱听）
- **可维护性**：把重复逻辑从业务代码里抽出来，避免到处复制粘贴。
- **可组合**：一个类/方法可以叠多个装饰器，形成清晰的能力栈（鉴权→埋点→缓存→错误处理）。
- **可测试**：核心逻辑仍是普通函数/类；装饰器只做“声明和注册”，更容易 mock。

---

### 被追问时的短答（不展开也显得懂）
- **装饰器和 React 有关系吗？**  
  React 里现在更主流是 hooks；装饰器更多在框架/工程层（Nest、MobX、路由、状态、AOP）用，我理解其适用边界。
- **什么时候不用装饰器？**  
  当团队构建链不统一、或者装饰器会让调试成本上升时，我会更倾向用显式函数封装（高阶函数/中间件）实现同样的横切能力。

因为 interface 运行时不存在，所以想“根据构造函数参数类型自动推断注入”时，参数如果写的是 interface，运行时只会变成 Object，容器就不知道该注入谁；而 class/abstract class 运行时存在，更容易作为注入 token。